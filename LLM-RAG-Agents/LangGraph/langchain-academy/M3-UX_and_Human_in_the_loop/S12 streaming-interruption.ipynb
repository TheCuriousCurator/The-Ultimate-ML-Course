{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and paste the API key in the prompt\n",
    "import os\n",
    "import configparser\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read('../../.config.ini')\n",
    "os.environ['GOOGLE_API_KEY'] = config.get('GoogleGeminiAPI', 'Google_Gemini_API_KEY')\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = config.get('LangChain', 'LANGCHAIN_TRACING_V2')\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = config.get('LangChain', 'LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = config.get('LangChain', 'LANGCHAIN_API_KEY')\n",
    "os.environ['USER_AGENT'] = config.get('LangChain', 'USER_AGENT')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config.get('OpenAI', 'OPENAI_API_KEY')\n",
    "os.environ['TAVILY_API_KEY'] = config.get('Tavily_Search_API', 'Tavily_API_KEY')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content='Hello Lance! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-c34cf820-b4a4-414c-a4b5-5e073a13c057-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! How's it going? What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "---------------------------------------------------------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Lance! How can I assist you today?\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event['messages']:\n",
    "        m.pretty_print()\n",
    "    print(\"---\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: __start__. Type: on_chain_start. Name: __start__\n",
      "Node: __start__. Type: on_chain_end. Name: __start__\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chain_start. Name: _write\n",
      "Node: conversation. Type: on_chain_end. Name: _write\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' professional', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' American', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Area', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' compete', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='NFL', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' league', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='N', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' founded', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' charter', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' All', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-Amer', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ica', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='AA', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' joined', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' when', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' leagues', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' merged', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Key', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Points', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='St', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='adium', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' play', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' games', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Levi', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Stadium', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Santa', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Clara', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' California', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' which', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' moved', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Before', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' played', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Cand', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='lestick', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Park', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='Team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Colors', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' colors', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' red', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' gold', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' white', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='Masc', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ot', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=\"'\", additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' mascot', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' S', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ourd', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ough', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Sam', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='Champ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ionship', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' five', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' titles', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='X', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='VI', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' XIX', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='III', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='IV', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='IX', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' period', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' being', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' early', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' also', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' numerous', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' titles', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' conference', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' championships', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='Not', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='able', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Players', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' had', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Hall', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Fame', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Joe', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Jerry', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Steve', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Young', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Ronnie', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' L', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ott', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Charles', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Haley', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' among', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' others', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='Co', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='aching', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Management', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' led', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' coaches', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Bill', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' who', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' credited', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' popular', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='izing', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' \"', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='West', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Coast', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Off', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ense', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\"', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' As', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='202', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='3', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' season', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' head', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' coach', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Kyle', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Shan', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ahan', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='R', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ival', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' rival', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' teams', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' such', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Dallas', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Cowboys', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Los', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Angeles', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Rams', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Seattle', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Seahawks', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='Ownership', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' owned', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' York', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' family', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' Jed', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' York', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' serving', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' CEO', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' rich', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' passionate', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' fan', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' base', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' competitive', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' various', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' eras', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' continue', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' be', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' prominent', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc'}, id='run-f45c7d0c-f5ab-46dc-af78-b86ce296e1be')}\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| The| team| was| founded| in| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n",
      "\n",
      "|###| Key| Points| about| the| |49|ers|:\n",
      "\n",
      "|-| **|St|adium|**|:| The| |49|ers| play| their| home| games| at| Levi|'s| Stadium| in| Santa| Clara|,| California|,| which| they| moved| to| in| |201|4|.| Before| that|,| they| played| at| Cand|lestick| Park| in| San| Francisco|.\n",
      "\n",
      "|-| **|Team| Colors|**|:| The| team's| colors| are| red|,| gold|,| and| white|.\n",
      "\n",
      "|-| **|Champ|ionship|s|**|:| The| |49|ers| have| won| five| Super| Bowl| titles| (|X|VI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|),| with| their| most| successful| period| being| the| |198|0|s| and| early| |199|0|s|.| They| have| also| won| numerous| division| titles| and| conference| championships|.\n",
      "\n",
      "|-| **|Not|able| Players|**|:| The| team| has| had| several| Hall| of| Fame| players|,| including| Joe| Montana|,| Jerry| Rice|,| Steve| Young|,| Ronnie| L|ott|,| and| Charles| Haley|.| Jerry| Rice| is| often| considered| one| of| the| greatest| wide| receivers| in| NFL| history|.\n",
      "\n",
      "|-| **|Co|aching| and| Management|**|:| The| |49|ers| have| had| several| notable| head| coaches|,| including| Bill| Walsh|,| who| is| credited| with| popular|izing| the| West| Coast| offense|.| As| of| the| |202|3| season|,| the| head| coach| is| Kyle| Shan|ahan|.\n",
      "\n",
      "|-| **|R|ival|ries|**|:| The| |49|ers| have| intense| rival|ries| with| several| teams|,| most| notably| the| Dallas| Cowboys|,| Los| Angeles| Rams|,| and| Seattle| Seahawks|.\n",
      "\n",
      "|-| **|Recent| Performance|**|:| In| recent| years|,| the| |49|ers| have| been| competitive|,| reaching| the| Super| Bowl| in| the| |201|9| season| but| losing| to| the| Kansas| City| Chiefs|.| They| have| been| known| for| a| strong| defense| and| a| dynamic| offense| under| coach| Kyle| Shan|ahan|.\n",
      "\n",
      "|The| |49|ers| have| a| rich| history| and| a| passionate| fan| base|,| making| them| one| of| the| most| stor|ied| franchises| in| the| NFL|.||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "--\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
    "\n",
    "*Also, if you are running this notebook in CoLab, then skip this step.*\n",
    "\n",
    "--\n",
    "\n",
    "The LangGraph API [has first class support for streaming](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#streaming). \n",
    "\n",
    "Let's load our `agent` in the Studio UI, which uses `module-3/studio/agent.py` set in `module-3/studio/langgraph.json`.\n",
    "\n",
    "The LangGraph API serves as the back-end for Studio.\n",
    "\n",
    "We can interact directly with the LangGraph API via the LangGraph SDK.\n",
    "\n",
    "We just need to get the URL for the local deployment from Studio.\n",
    "\n",
    "![Screenshot 2024-08-27 at 2.20.34 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf8943c3d4df239cbf0f_streaming2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unfortunately LangGraph Studio is currently not supported on Google Colab or requires a Mac",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle.colab\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(get_ipython()) \u001b[38;5;129;01mor\u001b[39;00m platform\u001b[38;5;241m.\u001b[39msystem() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDarwin\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnfortunately LangGraph Studio is currently not supported on Google Colab or requires a Mac\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Unfortunately LangGraph Studio is currently not supported on Google Colab or requires a Mac"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "if 'google.colab' in str(get_ipython()) or platform.system() != 'Darwin':\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab or requires a Mac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_transports/default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_transports/default.py:377\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 377\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:216\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:196\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_async/connection.py:99\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_async/connection.py:76\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_async/connection.py:122\u001b[0m, in \u001b[0;36mAsyncHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_backends/auto.py:30\u001b[0m, in \u001b[0;36mAutoBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_backend()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\n\u001b[1;32m     31\u001b[0m     host,\n\u001b[1;32m     32\u001b[0m     port,\n\u001b[1;32m     33\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     34\u001b[0m     local_address\u001b[38;5;241m=\u001b[39mlocal_address,\n\u001b[1;32m     35\u001b[0m     socket_options\u001b[38;5;241m=\u001b[39msocket_options,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_backends/anyio.py:115\u001b[0m, in \u001b[0;36mAnyIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    110\u001b[0m exc_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    113\u001b[0m     anyio\u001b[38;5;241m.\u001b[39mBrokenResourceError: ConnectError,\n\u001b[1;32m    114\u001b[0m }\n\u001b[0;32m--> 115\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m get_client(url\u001b[38;5;241m=\u001b[39mURL)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Search all hosted graphs\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m assistants \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39massistants\u001b[38;5;241m.\u001b[39msearch()\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/langgraph_sdk/client.py:696\u001b[0m, in \u001b[0;36mAssistantsClient.search\u001b[0;34m(self, metadata, graph_id, limit, offset)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_id:\n\u001b[1;32m    695\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m graph_id\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/assistants/search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    698\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    699\u001b[0m )\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/langgraph_sdk/client.py:223\u001b[0m, in \u001b[0;36mHttpClient.post\u001b[0;34m(self, path, json)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     headers, content \u001b[38;5;241m=\u001b[39m {}, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 223\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(path, headers\u001b[38;5;241m=\u001b[39mheaders, content\u001b[38;5;241m=\u001b[39mcontent)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_client.py:1905\u001b[0m, in \u001b[0;36mAsyncClient.post\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1885\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1886\u001b[0m     url: URL \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1898\u001b[0m     extensions: RequestExtensions \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1899\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;124;03m    Send a `POST` request.\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \n\u001b[1;32m   1903\u001b[0m \u001b[38;5;124;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   1906\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1907\u001b[0m         url,\n\u001b[1;32m   1908\u001b[0m         content\u001b[38;5;241m=\u001b[39mcontent,\n\u001b[1;32m   1909\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1910\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m   1911\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[1;32m   1912\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m   1913\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1914\u001b[0m         cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[1;32m   1915\u001b[0m         auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1916\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1917\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1918\u001b[0m         extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m   1919\u001b[0m     )\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_client.py:1585\u001b[0m, in \u001b[0;36mAsyncClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1572\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m   1573\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   1574\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m   1584\u001b[0m )\n\u001b[0;32m-> 1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_client.py:1674\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1672\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1674\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1675\u001b[0m     request,\n\u001b[1;32m   1676\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1677\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1678\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1679\u001b[0m )\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_client.py:1702\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1699\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1702\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1703\u001b[0m         request,\n\u001b[1;32m   1704\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1705\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_client.py:1739\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1739\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1741\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_client.py:1776\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1773\u001b[0m     )\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1776\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1779\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_transports/default.py:376\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m    364\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    365\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    366\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    375\u001b[0m )\n\u001b[0;32m--> 376\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/conda_env/langchain_3.11/lib/python3.11/site-packages/httpx/_transports/default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     88\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed"
     ]
    }
   ],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# Replace this with the URL of your own deployed graph\n",
    "URL = \"http://localhost:56091\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '1ef6a3d0-41eb-66f4-a311-8ebdfa1b281f'})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4dd5ce10-ac0b-4a91-b34b-c35109dcbf29', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4dd5ce10-ac0b-4a91-b34b-c35109dcbf29', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}, {'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-b5862486-a25f-48fc-9a03-a8506a6692a8', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]})\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'example': False} id='f51807de-6b99-4da4-a798-26cf59d16412'\n",
      "=========================\n",
      "content='' additional_kwargs={'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_imZHAw7kvMR2ZeKaQVSlj25C', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': None} id='run-fa4ab1c6-274d-4be5-8c4a-a6411c7c35cc' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_imZHAw7kvMR2ZeKaQVSlj25C', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='6' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {}, 'status': 'success'} name='multiply' id='3e7bbfb6-aa82-453a-969c-9c753fbd1d74' tool_call_id='call_imZHAw7kvMR2ZeKaQVSlj25C'\n",
      "=========================\n",
      "content='The result of multiplying 2 and 3 is 6.' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': None} id='run-e8e0d672-cfb2-42be-850a-345df3718f69'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/complete\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/complete\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 1ef6a3da-687f-6253-915a-701de5327165\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "--------------------------------------------------\n",
      "AI: The result\n",
      "--------------------------------------------------\n",
      "AI: The result of\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
