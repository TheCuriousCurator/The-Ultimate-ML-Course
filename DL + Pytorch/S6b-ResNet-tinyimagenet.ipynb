{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ec80c9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002822,
     "end_time": "2025-01-26T06:10:23.172994",
     "exception": false,
     "start_time": "2025-01-26T06:10:23.170172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Extract and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee9a34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T06:10:34.787963Z",
     "iopub.status.busy": "2025-01-26T06:10:34.787653Z",
     "iopub.status.idle": "2025-01-26T06:10:43.627344Z",
     "shell.execute_reply": "2025-01-26T06:10:43.626246Z"
    },
    "papermill": {
     "duration": 8.845521,
     "end_time": "2025-01-26T06:10:43.629404",
     "exception": false,
     "start_time": "2025-01-26T06:10:34.783883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tiny-imagenet-200/train ./data/tiny-imagenet-200/val\n"
     ]
    }
   ],
   "source": [
    "# Notebook based on https://www.kaggle.com/code/tanvirnwu/simple-neural-network-training-imagenet\n",
    "\n",
    "# Imports of this notebook\n",
    "import os\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Set local Data directory for TinyImageneet\n",
    "data_dir = './data/tiny-imagenet-200'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "print(train_dir, val_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70ac15",
   "metadata": {
    "papermill": {
     "duration": 0.002377,
     "end_time": "2025-01-26T06:10:43.634348",
     "exception": false,
     "start_time": "2025-01-26T06:10:43.631971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Define a Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4efc6da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T06:10:43.641191Z",
     "iopub.status.busy": "2025-01-26T06:10:43.640310Z",
     "iopub.status.idle": "2025-01-26T06:10:43.651974Z",
     "shell.execute_reply": "2025-01-26T06:10:43.650957Z"
    },
    "papermill": {
     "duration": 0.016883,
     "end_time": "2025-01-26T06:10:43.653728",
     "exception": false,
     "start_time": "2025-01-26T06:10:43.636845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Simple Tiny Imagenet Dataloader\"\"\"\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        \n",
    "        if self.train:\n",
    "            self.data = []\n",
    "            self.labels = []\n",
    "            classes = sorted(os.listdir(os.path.join(root_dir, 'train')))\n",
    "            \n",
    "            # Search all images in disk and set the label as the index of the directory.\n",
    "            for label, cls in enumerate(classes):\n",
    "                cls_dir = os.path.join(root_dir, 'train', cls, 'images')\n",
    "                for img_name in os.listdir(cls_dir):\n",
    "                    self.data.append(os.path.join(cls_dir, img_name)) # store the path only\n",
    "                    self.labels.append(label)\n",
    "        else:\n",
    "            self.data = []\n",
    "            self.labels = []\n",
    "            val_dir = os.path.join(root_dir, 'val', 'images')\n",
    "            \n",
    "            # ground-truth for validation data is stored on cvs files\n",
    "            val_annotations = pd.read_csv(os.path.join(root_dir, 'val', 'val_annotations.txt'), \n",
    "                                          sep='\\t', header=None, \n",
    "                                          names=['file_name', 'class', 'x1', 'y1', 'x2', 'y2']) \n",
    "            class_to_idx = {cls: idx for idx, cls in enumerate(sorted(os.listdir(os.path.join(root_dir, 'train'))))}\n",
    "            for _, row in val_annotations.iterrows():\n",
    "                self.data.append(os.path.join(val_dir, row['file_name'])) # again store the path only\n",
    "                self.labels.append(class_to_idx[row['class']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        # Load image and fetch labels\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply transform if available\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daca922",
   "metadata": {
    "papermill": {
     "duration": 0.002201,
     "end_time": "2025-01-26T06:10:43.658284",
     "exception": false,
     "start_time": "2025-01-26T06:10:43.656083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3 Instantiate Network, transformations and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1519d969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T06:10:43.664403Z",
     "iopub.status.busy": "2025-01-26T06:10:43.663863Z",
     "iopub.status.idle": "2025-01-26T06:10:46.889230Z",
     "shell.execute_reply": "2025-01-26T06:10:46.888436Z"
    },
    "papermill": {
     "duration": 3.230691,
     "end_time": "2025-01-26T06:10:46.891319",
     "exception": false,
     "start_time": "2025-01-26T06:10:43.660628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mb600l/conda_env/pytorch_GPU_3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mb600l/conda_env/pytorch_GPU_3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Defining the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Loading the dataset\n",
    "data_dir = data_dir\n",
    "train_dataset = TinyImageNetDataset(root_dir=data_dir, transform=transform, train=True)\n",
    "val_dataset = TinyImageNetDataset(root_dir=data_dir, transform=transform, train=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=4)\n",
    "\n",
    "# Cuda Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device is', device)\n",
    "\n",
    "# Resnet18 imagente pretraining\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "# Adapt the final layer for Tiny Imagenet\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 200)\n",
    "\n",
    "# Loss fucntion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250d0f2",
   "metadata": {
    "papermill": {
     "duration": 0.002468,
     "end_time": "2025-01-26T06:10:46.896500",
     "exception": false,
     "start_time": "2025-01-26T06:10:46.894032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b153638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T06:10:46.903131Z",
     "iopub.status.busy": "2025-01-26T06:10:46.902291Z",
     "iopub.status.idle": "2025-01-26T06:20:23.509821Z",
     "shell.execute_reply": "2025-01-26T06:20:23.508407Z"
    },
    "papermill": {
     "duration": 576.612995,
     "end_time": "2025-01-26T06:20:23.511852",
     "exception": false,
     "start_time": "2025-01-26T06:10:46.898857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/196 [00:00<?, ?it/s]/home/mb600l/conda_env/pytorch_GPU_3.9/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608883701/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training: 100%|██████████| 196/196 [00:19<00:00, 10.27it/s, loss=4.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.1559\n",
      "Epoch [1/10], Train Loss: 4.2811, Val Loss: 3.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.50it/s, loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2217\n",
      "Epoch [2/10], Train Loss: 3.4055, Val Loss: 3.4839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.42it/s, loss=2.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2741\n",
      "Epoch [3/10], Train Loss: 2.9749, Val Loss: 3.1589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.48it/s, loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3005\n",
      "Epoch [4/10], Train Loss: 2.6095, Val Loss: 3.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.54it/s, loss=2.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3009\n",
      "Epoch [5/10], Train Loss: 2.2550, Val Loss: 3.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.55it/s, loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3175\n",
      "Epoch [6/10], Train Loss: 1.8591, Val Loss: 3.1178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:16<00:00, 11.70it/s, loss=1.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3042\n",
      "Epoch [7/10], Train Loss: 1.4182, Val Loss: 3.3225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.62it/s, loss=0.944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2918\n",
      "Epoch [8/10], Train Loss: 0.9443, Val Loss: 3.7727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.85it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2904\n",
      "Epoch [9/10], Train Loss: 0.5322, Val Loss: 3.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 196/196 [00:18<00:00, 10.83it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3041\n",
      "Epoch [10/10], Train Loss: 0.2531, Val Loss: 4.2051\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    #for images, labels in train_loader:\n",
    "    #for images, labels in tqdm(train_loader, desc=\"Training Progress\", leave=True):\n",
    "    with tqdm(train_loader, desc=\"Training\", leave=True) as pbar:\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)          \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            pbar.set_postfix(loss=avg_loss)\n",
    "            \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    accuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=200).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Update metric\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            accuracy_metric.update(predictions, labels)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    validation_accuracy = accuracy_metric.compute()\n",
    "    print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 56828,
     "sourceId": 109264,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 605.68979,
   "end_time": "2025-01-26T06:20:26.013719",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-26T06:10:20.323929",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
