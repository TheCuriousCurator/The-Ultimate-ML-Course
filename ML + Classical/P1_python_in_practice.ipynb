{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning APIs with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newsgroups_model.joblib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load some categories of newsgroups dataset\n",
    "categories = [\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"sci.crypt\",\n",
    "]\n",
    "\n",
    "newsgroups_training = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, random_state=0\n",
    ")\n",
    "newsgroups_testing = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, random_state=0\n",
    ")\n",
    "\n",
    "# Make the pipeline\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    MultinomialNB(),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(newsgroups_training.data, newsgroups_training.target)\n",
    "\n",
    "# Serialize the model and the target names\n",
    "model_file = \"newsgroups_model.joblib\"\n",
    "model_targets_tuple = (model, newsgroups_training.target_names)\n",
    "joblib.dump(model_targets_tuple, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the model\n",
    "model_file = os.path.join(\"newsgroups_model.joblib\")\n",
    "loaded_model: Tuple[Pipeline, List[str]] = joblib.load(model_file)\n",
    "model, targets = loaded_model\n",
    "\n",
    "# Run a prediction\n",
    "p = model.predict([\"computer cpu memory ram\"])\n",
    "print(targets[p[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prediction_endpoint.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction_endpoint.py\n",
    "# Fast API endpoint \n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import joblib\n",
    "from fastapi import FastAPI, Depends, status\n",
    "from pydantic import BaseModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "\n",
    "class PredictionOutput(BaseModel):\n",
    "    category: str\n",
    "\n",
    "\n",
    "memory = joblib.Memory(location=\"cache.joblib\")\n",
    "\n",
    "@memory.cache(ignore=[\"model\"])\n",
    "def predict(model: Pipeline, text: str) -> int:\n",
    "    prediction = model.predict([text])\n",
    "    return prediction[0]\n",
    "\n",
    "class NewsgroupsModel:\n",
    "    model: Optional[Pipeline]\n",
    "    targets: Optional[List[str]]\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the model\"\"\"\n",
    "        model_file = os.path.join(os.path.dirname(__file__), \"newsgroups_model.joblib\")\n",
    "        loaded_model: Tuple[Pipeline, List[str]] = joblib.load(model_file)\n",
    "        model, targets = loaded_model\n",
    "        self.model = model\n",
    "        self.targets = targets\n",
    "\n",
    "    async def predict(self, input: PredictionInput) -> PredictionOutput:\n",
    "        \"\"\"Runs a prediction\"\"\"\n",
    "        if not self.model or not self.targets:\n",
    "            raise RuntimeError(\"Model is not loaded\")\n",
    "        #prediction = self.model.predict([input.text])\n",
    "        prediction = predict(self.model, input.text)\n",
    "        #category = self.targets[prediction[0]]\n",
    "        category = self.targets[prediction]\n",
    "        return PredictionOutput(category=category)\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "newgroups_model = NewsgroupsModel()\n",
    "\n",
    "\n",
    "@app.post(\"/prediction\")\n",
    "async def prediction(\n",
    "    output: PredictionOutput = Depends(newgroups_model.predict),\n",
    ") -> PredictionOutput:\n",
    "    return output\n",
    "\n",
    "\n",
    "@app.delete(\"/cache\", status_code=status.HTTP_204_NO_CONTENT)\n",
    "def delete_cache():\n",
    "    memory.clear()\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    newgroups_model.load_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using API from terminal \n",
    "uvicorn prediction_endpoint:app --reload\n",
    "\n",
    "# predict from text input\n",
    "curl -X 'POST' \\\n",
    "  'http://127.0.0.1:8000/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"text\": \"ram\"\n",
    "}'\n",
    "\n",
    "# Clear Cache\n",
    "curl -X 'DELETE' \\\n",
    "  'http://127.0.0.1:8000/cache' \\\n",
    "  -H 'accept: */*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using API from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'comp.sys.mac.hardware'}\n",
      "{'category': 'soc.religion.christian'}\n",
      "{'category': 'comp.sys.mac.hardware'}\n",
      "{'category': 'sci.crypt'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "input_data_batch = [{\"text\": \"computer ram\"},\n",
    "             {\"text\": \"I love bible\"},\n",
    "             {\"text\": \"ram\"},\n",
    "             {\"text\": \"I love cryptography \"}\n",
    "             ]\n",
    "\n",
    "for data in input_data_batch:\n",
    "    response = requests.post(f\"{base_url}/prediction/\", json=data)\n",
    "    print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "response = requests.delete(f\"{base_url}/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
