{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE/OPL Experiments with Synthetic Bandit Data\n",
    "---\n",
    "This notebook provides an example of conducting OPE of several different evaluation policies with synthetic bandit feedback data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_behavior_policy,\n",
    ")\n",
    "from obp.policy import IPWLearner\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.5\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Generate Synthetic Data\n",
    "\n",
    "`SyntheticBanditDataset` is an easy-to-use synthetic data generator class in the dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10, # number of actions; |A|\n",
    "    dim_context=5, # number of dimensions of context vector\n",
    "    reward_function=logistic_reward_function, # mean reward function; q(x,a)\n",
    "    behavior_policy_function=linear_behavior_policy, # behavior policy; \\pi_b\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bandit_data = dataset.obtain_batch_bandit_feedback(n_rounds=10000)\n",
    "test_bandit_data = dataset.obtain_batch_bandit_feedback(n_rounds=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rounds': 10000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],\n",
       "        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],\n",
       "        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]],\n",
       "       shape=(10000, 5)),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([8, 3, 2, ..., 0, 3, 7], shape=(10000,)),\n",
       " 'position': None,\n",
       " 'reward': array([1, 1, 1, ..., 0, 1, 0], shape=(10000,)),\n",
       " 'expected_reward': array([[0.97118016, 0.87577106, 0.59611873, ..., 0.87533511, 0.84390015,\n",
       "         0.59546542],\n",
       "        [0.78825808, 0.44831018, 0.72179621, ..., 0.91014419, 0.81574968,\n",
       "         0.85934013],\n",
       "        [0.71911111, 0.68382756, 0.61755339, ..., 0.9294842 , 0.75276165,\n",
       "         0.93292626],\n",
       "        ...,\n",
       "        [0.98088853, 0.85829219, 0.21894522, ..., 0.80550994, 0.95587212,\n",
       "         0.22918765],\n",
       "        [0.95956626, 0.91529075, 0.62826258, ..., 0.69379095, 0.88489089,\n",
       "         0.73264105],\n",
       "        [0.98834976, 0.95130073, 0.97439115, ..., 0.52016775, 0.50918384,\n",
       "         0.97920376]], shape=(10000, 10)),\n",
       " 'pi_b': array([[[0.07223028],\n",
       "         [0.09909056],\n",
       "         [0.07096979],\n",
       "         ...,\n",
       "         [0.11253091],\n",
       "         [0.07897588],\n",
       "         [0.06885699]],\n",
       " \n",
       "        [[0.05409571],\n",
       "         [0.07094081],\n",
       "         [0.09145125],\n",
       "         ...,\n",
       "         [0.0696243 ],\n",
       "         [0.09055824],\n",
       "         [0.1195411 ]],\n",
       " \n",
       "        [[0.05810528],\n",
       "         [0.07653797],\n",
       "         [0.07867303],\n",
       "         ...,\n",
       "         [0.08396879],\n",
       "         [0.07859185],\n",
       "         [0.13944755]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.0783432 ],\n",
       "         [0.09641408],\n",
       "         [0.07635862],\n",
       "         ...,\n",
       "         [0.12490613],\n",
       "         [0.0622832 ],\n",
       "         [0.05966726]],\n",
       " \n",
       "        [[0.07718357],\n",
       "         [0.07998376],\n",
       "         [0.0670145 ],\n",
       "         ...,\n",
       "         [0.12967892],\n",
       "         [0.08519538],\n",
       "         [0.08503295]],\n",
       " \n",
       "        [[0.07246781],\n",
       "         [0.06425243],\n",
       "         [0.05193479],\n",
       "         ...,\n",
       "         [0.09758413],\n",
       "         [0.11348707],\n",
       "         [0.07819554]]], shape=(10000, 10, 1)),\n",
       " 'pscore': array([0.07897588, 0.17829834, 0.07867303, ..., 0.0783432 , 0.08365078,\n",
       "        0.09758413], shape=(10000,))}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_bandit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train Bandit Policies (OPL)\n",
    "`obp.policy.IPWLearner` can be a first choice (IPS=IPW)\n",
    "\n",
    "$ \\hat{\\pi} \\in \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\hat{V}_{I P S}\\left(\\pi ; \\mathcal{D}_{t r}\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipw_learner = IPWLearner(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345) # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "ipw_learner.fit(\n",
    "    context=training_bandit_data[\"context\"], # context; x\n",
    "    action=training_bandit_data[\"action\"], # action; a\n",
    "    reward=training_bandit_data[\"reward\"], # reward; r\n",
    "    pscore=training_bandit_data[\"pscore\"], # propensity score; pi_b(a|x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (action dist = action distribution)\n",
    "action_dist_ipw = ipw_learner.predict(\n",
    "    context=test_bandit_data[\"context\"], # context in the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(1000000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dist_ipw[:, :, 0] # which action to take for each context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi(a|x_i)} [r(x_i, a)], \\; \\, where \\; \\, r(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_value_of_ipw = dataset.calc_ground_truth_policy_value(\n",
    "    expected_reward=test_bandit_data[\"expected_reward\"], # expected rewards; q(x,a)\n",
    "    action_dist=action_dist_ipw, # action distribution of IPWLearner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9513585062082754)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth policy value of `IPWLearner`\n",
    "policy_value_of_ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Off-Policy Evaluation (OPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (4-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$r(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{r}(x,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_model=LogisticRegression(C=100, random_state=12345) # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=test_bandit_data[\"context\"], # context; x\n",
    "    action=test_bandit_data[\"action\"], # action; a\n",
    "    reward=test_bandit_data[\"reward\"], # reward; r\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87875364, 0.85361329, 0.76432741, ..., 0.73185514, 0.89724672,\n",
       "        0.8509608 ],\n",
       "       [0.83965945, 0.80818239, 0.70089619, ..., 0.66353144, 0.86318702,\n",
       "        0.80489485],\n",
       "       [0.83176297, 0.79910661, 0.68869948, ..., 0.6505707 , 0.85625086,\n",
       "        0.79570284],\n",
       "       ...,\n",
       "       [0.76701108, 0.72592735, 0.59565255, ..., 0.5535156 , 0.79864233,\n",
       "        0.72171548],\n",
       "       [0.74290042, 0.69923232, 0.5638916 , ..., 0.52110587, 0.77685269,\n",
       "        0.69478271],\n",
       "       [0.87502747, 0.84924705, 0.75805289, ..., 0.7250271 , 0.89402035,\n",
       "        0.84652967]], shape=(1000000, 10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{q}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta)$ using DM, IPS, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=test_bandit_data, # test data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"),\n",
    "    ] # used estimators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist_ipw, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}(x,a)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': np.float64(0.9517002919680564),\n",
       " 'DM': np.float64(0.7774587683848764),\n",
       " 'DR': np.float64(0.952131301877404)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators\n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_0) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=policy_value_of_ipw, # V(\\pi_e)\n",
    "    action_dist=action_dist_ipw, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}(x,a)\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': np.float64(1.1681750558903697e-07),\n",
       " 'DM': np.float64(0.030241118815046934),\n",
       " 'DR': np.float64(5.972131462238877e-07)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # DR is the most accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_0^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_0^{(t)}$ is the synthetic data in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
