{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE Experiment with Open Bandit Dataset\n",
    "---\n",
    "This notebook demonstrates an example of conducting OPE of Bernoulli Thompson Sampling (BernoulliTS) as an evaluation policy using some OPE estimators and logged bandit feedback generated by running the Random policy (behavior policy) on the ZOZOTOWN platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.5\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Loading and Preprocessing\n",
    "\n",
    "`obp.dataset.OpenBanditDataset` is an easy-to-use data loader for Open Bandit Dataset. \n",
    "\n",
    "It takes behavior policy ('bts' or 'random') and campaign ('all', 'men', or 'women') as inputs and provides dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n",
    "dataset = OpenBanditDataset(behavior_policy='random', campaign='all', data_path='./open_bandit_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain logged bandit data generated by behavior policy\n",
    "bandit_data = dataset.obtain_batch_bandit_feedback()\n",
    "\n",
    "# `bandit_data` is a dictionary storing logged bandit feedback\n",
    "bandit_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's see some properties of the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obd'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the dataset is 'obd' (open bandit dataset)\n",
    "dataset.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of actions of the \"All\" campaign is 80\n",
    "dataset.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374327"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small example dataset has 10,000 rounds\n",
    "dataset.n_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default context (feature) engineering creates context vector with 20 dimensions\n",
    "dataset.dim_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZOZOTOWN recommendation interface has three positions\n",
    "# (please see https://github.com/st-tech/zr-obp/blob/master/images/recommended_fashion_items.png)\n",
    "dataset.len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Production Policy Replication\n",
    "\n",
    "After preparing the dataset, we now replicate the BernoulliTS policy implemented on the ZOZOTOWN recommendation interface during the data collection period.\n",
    "\n",
    "Here, we use `obp.policy.BernoulliTS` as an evaluation policy. \n",
    "By activating its `is_zozotown_prior` argument, we can replicate (the policy parameters of) BernoulliTS used in ZOZOTOWN production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_policy = BernoulliTS(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
    "    is_zozotown_prior=True, # replicate the BernoulliTS policy in the ZOZOTOWN production\n",
    "    campaign=\"all\",\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the action choice probabilities of the evaluation policy using Monte Carlo simulation\n",
    "action_dist = evaluation_policy.compute_batch_action_dist(\n",
    "    n_sim=100000, n_rounds=bandit_data[\"n_rounds\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]]], shape=(5, 80, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action_dist is an array of shape (n_rounds, n_actions, len_list) \n",
    "# representing the distribution over actions by the evaluation policy\n",
    "action_dist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "\n",
    "The next step is **OPE**, which attempts to estimate the performance of new decision making policies using only log data generated by behavior, past policies. \n",
    "\n",
    "Here, we use\n",
    "- **Inverse Propensity Score (IPS)**\n",
    "- **DirectMethod (DM)**\n",
    "- **Doubly Robust (DR)**\n",
    "\n",
    "to estimate the performance of Bernoulli TS using only the log data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$r(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{r}(x,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obp.ope.RegressionModel\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
    "    base_model=LogisticRegression(C=100, max_iter=10000, random_state=12345), # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=bandit_data[\"context\"],\n",
    "    action=bandit_data[\"action\"],\n",
    "    reward=bandit_data[\"reward\"],\n",
    "    position=bandit_data[\"position\"],\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00323218, 0.00234623, 0.00217014, ..., 0.00214775, 0.00443273,\n",
       "        0.00330252],\n",
       "       [0.00323218, 0.00234623, 0.00217014, ..., 0.00214775, 0.00443273,\n",
       "        0.00330252],\n",
       "       [0.00323218, 0.00234623, 0.00217014, ..., 0.00214775, 0.00443273,\n",
       "        0.00330252],\n",
       "       ...,\n",
       "       [0.00292821, 0.0021254 , 0.00196585, ..., 0.00194557, 0.00401631,\n",
       "        0.00299196],\n",
       "       [0.00292821, 0.0021254 , 0.00196585, ..., 0.00194557, 0.00401631,\n",
       "        0.00299196],\n",
       "       [0.00292821, 0.0021254 , 0.00196585, ..., 0.00194557, 0.00401631,\n",
       "        0.00299196]], shape=(1374327, 80))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{q}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta)$ using DM, IPS, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_data, # bandit data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"),\n",
    "    ] # used estimators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': np.float64(0.004693745811586323),\n",
       " 'DM': np.float64(0.004659478409397049),\n",
       " 'DR': np.float64(0.004676722146763697)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Evaluation of OPE\n",
    "\n",
    "Our final step is the **evaluation of OPE**, which evaluates the OPE performance (estimation accuracy) of the OPE estimators.\n",
    "\n",
    "Specifically, we asses the accuracy of the estimators by comparing their estimation with the ground-truth policy value estimated via the on-policy estimation from Open Bandit Dataset.\n",
    "\n",
    "This type evaluation of OPE is possible, because Open Bandit Dataset contains a set of *multiple* different logged bandit datasets collected by running different policies on the same platform at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} r_i, $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first calculate the ground-truth policy value of the evaluation policy\n",
    "# , which is estimated by averaging the factual (observed) rewards contained in the dataset (on-policy estimation)\n",
    "policy_value_bts = OpenBanditDataset.calc_on_policy_policy_value_estimate(\n",
    "    behavior_policy='bts', campaign='all', data_path='./open_bandit_dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.004953225649823585)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_value_bts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators \n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_0) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_0, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=policy_value_bts,\n",
    "    action_dist=action_dist,\n",
    "    estimated_rewards_by_reg_model=estimated_rewards,\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': np.float64(6.732978645163549e-08),\n",
       " 'DM': np.float64(8.628744125820509e-08),\n",
       " 'DR': np.float64(7.64541872043895e-08)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # IPS is the most accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_0^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_0^{(t)}$ is the synthetic data in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the OPE demonstration here is with the small size example version of our dataset. \n",
    "Please use its full size version (https://research.zozo.com/data.html) to produce more reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
