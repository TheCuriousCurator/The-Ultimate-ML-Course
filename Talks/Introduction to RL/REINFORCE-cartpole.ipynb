{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82308d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7179c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d778957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make environment, check spaces, get obs / act dims\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "#env = gym.make(env_name, render_mode=\"human\")\n",
    "assert isinstance(env.observation_space, Box), \\\n",
    "    \"This example only works for envs with continuous state spaces.\"\n",
    "assert isinstance(env.action_space, Discrete), \\\n",
    "    \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n\n",
    "hidden_sizes=[32]\n",
    "logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, lr=1e-2, \n",
    "          epochs=250, batch_size=5000, render=True):\n",
    "    \n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        obs = obs[0] if isinstance(obs, tuple) else obs  # handle gymnasium reset return\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            #print(obs)\n",
    "            batch_obs.append(deepcopy(obs))\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            # observation, reward, terminated, truncated, info\n",
    "            obs, rew, terminated, truncated, info = env.step(act)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "                obs = obs[0] if isinstance(obs, tuple) else obs  # handle gymnasium reset return\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, render=True, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env_name='CartPole-v1', logits_net=logits_net, epochs=10, render=True):\n",
    "    logits_net.eval()  # set logits_net to eval mode, so it doesn't track gradients\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "    \n",
    "    # for training policy\n",
    "    def test_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        obs = obs[0] if isinstance(obs, tuple) else obs  # handle gymnasium reset return\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            #print(obs)\n",
    "            batch_obs.append(deepcopy(obs))\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            # observation, reward, terminated, truncated, info\n",
    "            obs, rew, terminated, truncated, info = env.step(act)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "                obs = obs[0] if isinstance(obs, tuple) else obs  # handle gymnasium reset return\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "                batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "                return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # testing loop\n",
    "    r = 0\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = test_one_epoch()\n",
    "        r = r + np.mean(batch_rets)\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "    print(f\"Average Total return over {epochs} epochs: {r/epochs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(env_name='CartPole-v1', logits_net=logits_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa30b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
