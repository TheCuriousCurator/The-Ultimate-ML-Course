{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea49531",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session #2: Everything you need to know about Transformer deployment (200)\n",
    "\n",
    "In this session, you will learn about the different inference options are and how to use them.\n",
    "\n",
    "As of today Amazon SageMaker offers 4 different inference options with: \n",
    "\n",
    "* [Real-Time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)\n",
    "* [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n",
    "* [Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)\n",
    "* [Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)\n",
    "\n",
    "Each of these inference options has different characteristics and use cases. Therefore we have created a table to compare the current existing SageMaker inference in latency, execution periode, payload, size and pricing and getting-started examples on how to use each of the inference options.\n",
    "\n",
    "**Comparison table**\n",
    "\n",
    "| Option          | latency budget | execution period        | max payload size | real-world example      | accelerators (GPU) | pricing                                                       |\n",
    "|-----------------|----------------|-------------------------|------------------|-------------------------|--------------------|---------------------------------------------------------------|\n",
    "| real-time       | milliseconds   | constantly              | 6MB              | route estimation        | Yes                | up time of the endpoint                                       |\n",
    "| batch transform | hours          | ones a day/week         | Unlimited        | nightly embedding jobs  | Yes                | prediction (transform) time                                   |\n",
    "| async inference | minutes        | every few minutes/hours | 1GB              | post-call transcription | Yes                | up time of the endpoint, can sacle to 0 when there is no load |\n",
    "| serverless      | seconds        | every few minutes       | 6MB              | PoC for classification  | No                 | compute time (serverless)                                     |                                 |\n",
    "\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Deploy a Hugging Face Transformers For Real-Time inference.\n",
    "2. Deploy a Hugging Face Transformers for Batch Transform Inference.\n",
    "3. Deploy a Hugging Face Transformers for Asynchronous Inference.\n",
    "4. Deploy a Hugging Face Transformers for Serverless Inference.\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c00ca2",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8babd98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e23f3",
   "metadata": {},
   "source": [
    "## [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)\n",
    "\n",
    "The SageMaker Hugging Face Inference Toolkit is an open-source library for serving ðŸ¤— Transformers models on Amazon SageMaker. This library provides default pre-processing, predict and postprocessing for certain ðŸ¤— Transformers models and tasks using the `transformers pipelines`.\n",
    "The Inference Toolkit accepts inputs in the `inputs` key, and supports additional pipelines `parameters` in the parameters key. You can provide any of the supported kwargs from `pipelines` as `parameters`.\n",
    "\n",
    "Tasks supported by the Inference Toolkit API include:\n",
    "\n",
    "- **`text-classification`**\n",
    "- **`sentiment-analysis`**\n",
    "- **`token-classification`**\n",
    "- **`feature-extraction`**\n",
    "- **`fill-mask`**\n",
    "- **`summarization`**\n",
    "- **`translation_xx_to_yy`**\n",
    "- **`text2text-generation`**\n",
    "- **`text-generation`**\n",
    "- **`audio-classificatin`**\n",
    "- **`automatic-speech-recognition`**\n",
    "- **`conversational`**\n",
    "- **`image-classification`**\n",
    "- **`image-segmentation`**\n",
    "- **`object-detection`**\n",
    "- **`table-question-answering`**\n",
    "- **`zero-shot-classification`**\n",
    "- **`zero-shot-image-classification`**\n",
    "\n",
    "See the following request examples for some of the tasks:\n",
    "\n",
    "**text-classification**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**text-generation parameterized**\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"inputs\": \"Hugging Face, the winner of VentureBeatâ€™s Innovation in Natural Language Process/Understanding Award for 2021, is looking to level the playing field. The team, launched by ClÃ©ment Delangue and Julien Chaumond in 2016, was recognized for its work in democratizing NLP, the global market value for which is expected to hit $35.1 billion by 2026. This week, Googleâ€™s former head of Ethical AI Margaret Mitchell joined the team.\",\n",
    "  \"parameters\": {\n",
    "    \"repetition_penalty\": 4.0,\n",
    "    \"length_penalty\": 1.5\n",
    "  }\n",
    "}\n",
    "```\n",
    "More documentation and a list of supported tasks can be found in the [documentation](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6dbbf",
   "metadata": {},
   "source": [
    "## 1. Deploy a Hugging Face Transformers For Real-Time inference.\n",
    "\n",
    "### What are Amazon SageMaker Real-Time Endpoints? \n",
    "\n",
    "Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.\n",
    "\n",
    "**Deploying a model using SageMaker hosting services is a three-step process:**\n",
    "\n",
    "1. **Create a model in SageMaker** â€”By creating a model, you tell SageMaker where it can find the model components. \n",
    "2. **Create an endpoint configuration for an HTTPS endpoint** â€”You specify the name of one or more models in production variants and the ML compute instances that you want SageMaker to launch to host each production variant.\n",
    "3. **Create an HTTPS endpoint** â€”Provide the endpoint configuration to SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration\n",
    "\n",
    "![](../../imgs/sm-endpoint.png)\n",
    "\n",
    "\n",
    "### Deploy a Hugging Face Transformer from the [Hub](hf.co/models)\n",
    "\n",
    "Detailed Notebook: [deploy_model_from_hf_hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)\n",
    "\n",
    "To deploy a model directly from the Hub to SageMaker we need to define 2 environment variables when creating the `HuggingFaceModel` . We need to define:\n",
    "\n",
    "- `HF_MODEL_ID`: defines the model id, which will be automatically loaded from [huggingface.co/models](http://huggingface.co/models) when creating or SageMaker Endpoint. The ðŸ¤— Hub provides +14 000 models all available through this environment variable.\n",
    "- `HF_TASK`: defines the task for the used ðŸ¤— Transformers pipeline. A full list of tasks can be find [here](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaddfaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n",
    "  'HF_TASK':'question-answering' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model_rth = HuggingFaceModel(\n",
    "   env=hub, # hugging face hub configuration\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\", # transformers version used\n",
    "   pytorch_version=\"1.10\", # pytorch version used\n",
    "   py_version=\"py38\", # python version of the DLC\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor_rth = huggingface_model_rth.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.g4dn.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f833c",
   "metadata": {},
   "source": [
    "After model is deployed we can use the `predictor` to send requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db05249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9987210035324097, 'start': 68, 'end': 77, 'answer': 'sagemaker'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "\"inputs\": {\n",
    "    \"question\": \"What is used for inference?\",\n",
    "    \"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor_rth.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dd4f4",
   "metadata": {},
   "source": [
    "We can easily delete the endpoint again with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b9030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor_rth.delete_model()\n",
    "predictor_rth.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f96061",
   "metadata": {},
   "source": [
    "### Deploy a Hugging Face Transformer from the [Hub](hf.co/models)\n",
    "\n",
    "Detailed Notebook: [deploy_model_from_s3](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)\n",
    "\n",
    "To deploy a model directly from the Hub to SageMaker we need to define 2 environment variables when creating the `HuggingFaceModel` . We need to define:\n",
    "\n",
    "- `HF_MODEL_ID`: defines the model id, which will be automatically loaded from [huggingface.co/models](http://huggingface.co/models) when creating or SageMaker Endpoint. The ðŸ¤— Hub provides +14 000 models all available through this environment variable.\n",
    "- `HF_TASK`: defines the task for the used ðŸ¤— Transformers pipeline. A full list of tasks can be find [here](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbd8bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model_rts3 = HuggingFaceModel(\n",
    "   model_data=\"s3://hf-sagemaker-inference/model.tar.gz\",  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\", # transformers version used\n",
    "   pytorch_version=\"1.10\", # pytorch version used\n",
    "   py_version=\"py38\", # python version of the DLC\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor_rts3 = huggingface_model_rts3.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf4541",
   "metadata": {},
   "source": [
    "After model is deployed we can use the `predictor` to send requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9240af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996660947799683}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. I love it!\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor_rts3.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcbea5",
   "metadata": {},
   "source": [
    "We can easily delete the endpoint again with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7693bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor_rts3.delete_model()\n",
    "predictor_rts3.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4001e",
   "metadata": {},
   "source": [
    "## 2. Deploy a Hugging Face Transformers for Batch Transform Inference.\n",
    "\n",
    "Detailed Notebook: [batch_transform_inference](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)\n",
    "\n",
    "\n",
    "### What is Amazon SageMaker Batch Transform? \n",
    "\n",
    "A Batch transform job uses a trained model to get inferences on a dataset and saves these results to an Amazon S3 location that you specify. Similar to real-time hosting it creates a web server that takes in HTTP POST but additionally a Agent. The Agent reads the data from Amazon S3 and sends it to the web server and stores the prediction at the end back to Amazon S3. The benefit of Batch Transform is that the instances are only used during the \"job\" and stopped afterwards.\n",
    "\n",
    "![batch-transform](../../imgs/batch-transform-v2.png)\n",
    "\n",
    "\n",
    "**Use batch transform when you:**\n",
    "\n",
    "* Want to get inferences for an entire dataset and index them to serve inferences in real time\n",
    "* Don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "* Don't need the subsecond latency that SageMaker hosted endpoints provide\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e724605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "\n",
    "dataset_jsonl_file=\"./tweet_data.jsonl\"\n",
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"london/batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"london/batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file,input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'cardiffnlp/twitter-roberta-base-sentiment',\n",
    "    'HF_TASK':'text-classification'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,                        # configuration for loading model from Hub\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",    # transformers version used\n",
    "   pytorch_version=\"1.10\",         # pytorch version used\n",
    "   py_version='py38',              # python version used\n",
    ")\n",
    "\n",
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,              # number of instances used for running the batch job\n",
    "    instance_type='ml.m5.xlarge',# instance type for the batch job\n",
    "    output_path=output_s3_path,    # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord')       # How we are sending the \"requests\" to the endpoint\n",
    "\n",
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=s3_file_uri,               # preprocessed file location on s3 \n",
    "    content_type='application/json',# mime-type of the file    \n",
    "    split_type='Line')              # how the datapoints are split, here lines since it is `.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4001f",
   "metadata": {},
   "source": [
    "## 3. Deploy a Hugging Face Transformers for Asynchronous Inference.\n",
    "\n",
    "Detailed Notebook: [async_inference_hf_hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/16_async_inference_hf_hub/sagemaker-notebook.ipynb)\n",
    "\n",
    "### What is Amazon SageMaker Asynchronous Inference? \n",
    "\n",
    "Amazon SageMaker Asynchronous Inference is a new capability in SageMaker that queues incoming requests and processes them asynchronously. Compared to [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) [Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) provides immediate access to the results of the inference job rather than waiting for the job to complete.\n",
    "\n",
    "\n",
    "![async-inference](../../imgs/async-inference.png)\n",
    "\n",
    "\n",
    "**Whats the difference between batch transform & real-time inference:**\n",
    "\n",
    "* request will be uploaded to Amazon S3 and the Amazon S3 URI is passed in the request\n",
    "* are always up and running but can scale to zero to save costs\n",
    "* responses are also uploaded to Amazon S3 again.\n",
    "* you can create a Amazon SNS topic to recieve notifications when predictions are finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b889001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "from sagemaker.s3 import s3_path_join\n",
    "\n",
    "# Hub Model configuration. <https://huggingface.co/models>\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    'HF_TASK':'text-classification'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model_async = HuggingFaceModel(\n",
    "   env=hub,                      # configuration for loading model from Hub\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# create async endpoint configuration\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=s3_path_join(\"s3://\",sagemaker_session_bucket,\"async_inference/output\") , # Where our results will be stored\n",
    "    # notification_config={\n",
    "            #   \"SuccessTopic\": \"arn:aws:sns:us-east-2:123456789012:MyTopic\",\n",
    "            #   \"ErrorTopic\": \"arn:aws:sns:us-east-2:123456789012:MyTopic\",\n",
    "    # }, #  Notification configuration\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "async_predictor = huggingface_model_async.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    async_inference_config=async_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cf6a6",
   "metadata": {},
   "source": [
    "The `predict()` will upload our `data` to Amazon S3 and run inference against it. Since we are using `predict` it will block until the inference is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a06439a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998838901519775}, {'label': 'NEGATIVE', 'score': 0.999727189540863}, {'label': 'POSITIVE', 'score': 0.9998838901519775}, {'label': 'POSITIVE', 'score': 0.9994854927062988}]\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": [\n",
    "    \"it 's a charming and often affecting journey .\",\n",
    "    \"it 's slow -- very , very slow\",\n",
    "    \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "    \"the emotions are raw and will strike a nerve with anyone who 's ever had family trauma .\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "res = async_predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50601462",
   "metadata": {},
   "source": [
    "We can easily delete the endpoint again with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4378757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "async_predictor.delete_model()\n",
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4001d",
   "metadata": {},
   "source": [
    "## 4. Deploy a Hugging Face Transformers for Serverless Inference.\n",
    "\n",
    "Detailed Notebook: [serverless_inference](https://github.com/huggingface/notebooks/blob/main/sagemaker/19_serverless_inference/sagemaker-notebook.ipynb)\n",
    "\n",
    "### What is Amazon SageMaker Serverless Inference? \n",
    "\n",
    "[Amazon SageMaker Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html) is a purpose-built inference option that makes it easy for you to deploy and scale ML models. Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling.\n",
    "\n",
    "![serverless](../../imgs/serverless.png)\n",
    "\n",
    "\n",
    "**Use Severless Inference when you:**\n",
    "\n",
    "* Want to get started quickly in a cost-effective way\n",
    "* Don't need the subsecond latency that SageMaker hosted endpoints provide\n",
    "* proofs-of-concept where cold starts or scalability is not mission-critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f398d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker.serializers import DataSerializer\n",
    "\n",
    "# Hub Model configuration. <https://huggingface.co/models>\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'facebook/wav2vec2-base-960h',\n",
    "    'HF_TASK':'automatic-speech-recognition',\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model_sls = HuggingFaceModel(\n",
    "   env=hub,                      # configuration for loading model from Hub\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",        # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# Specify MemorySizeInMB and MaxConcurrency in the serverless config object\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=4096, max_concurrency=10,\n",
    ")\n",
    "\n",
    "# create a serializer for the data\n",
    "audio_serializer = DataSerializer(content_type='audio/x-audio') # using x-audio to support multiple audio formats\n",
    "\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor_sls = huggingface_model_sls.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    serializer=audio_serializer, # serializer for our audio data.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a06439a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-05 10:25:23--  https://cdn-media.huggingface.co/speech_samples/sample1.flac\n",
      "AuflÃ¶sen des Hostnamens cdn-media.huggingface.co (cdn-media.huggingface.co)â€¦ 13.227.153.12, 13.227.153.26, 13.227.153.89, ...\n",
      "Verbindungsaufbau zu cdn-media.huggingface.co (cdn-media.huggingface.co)|13.227.153.12|:443 â€¦ verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet â€¦ 200 OK\n",
      "LÃ¤nge: 282378 (276K) [audio/flac]\n",
      "Wird in Â»sample1.flacÂ« gespeichert.\n",
      "\n",
      "sample1.flac        100%[===================>] 275,76K  --.-KB/s    in 0,04s   \n",
      "\n",
      "2022-05-05 10:25:23 (6,18 MB/s) - Â»sample1.flacÂ« gespeichert [282378/282378]\n",
      "\n",
      "{'text': \"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS\"}\n"
     ]
    }
   ],
   "source": [
    "!wget https://cdn-media.huggingface.co/speech_samples/sample1.flac\n",
    "\n",
    "audio_path = \"sample1.flac\"\n",
    "\n",
    "res = predictor_sls.predict(data=audio_path)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50601463",
   "metadata": {},
   "source": [
    "We can easily delete the endpoint again with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4378757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor_sls.delete_model()\n",
    "predictor_sls.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6179b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
