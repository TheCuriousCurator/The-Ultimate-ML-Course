{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d2464c",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Deploy ðŸ¤— Transformers for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce3cec",
   "metadata": {},
   "source": [
    "Welcome to this getting started guide, we will use the new Hugging Face Inference DLCs and Amazon SageMaker Python SDK to deploy a transformer model for inference.  \n",
    "In this example we directly deploy one of the 10 000+ Hugging Face Transformers from the [Hub](https://huggingface.co/models) to Amazon SageMaker for Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7272df2",
   "metadata": {},
   "source": [
    "## API - [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daeacf3",
   "metadata": {},
   "source": [
    "Using the `transformers pipelines`, we designed an API, which makes it easy for you to benefit from all `pipelines` features. The API is oriented at the API of the [ðŸ¤—  Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html), meaning your inputs need to be defined in the `inputs` key and if you want additional supported `pipelines` parameters you can add them in the `parameters` key. Below you can find examples for requests. \n",
    "\n",
    "**text-classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "```\n",
    "**question-answering request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": {\n",
    "\t\t\"question\": \"What is used for inference?\",\n",
    "\t\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "**zero-shot classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
    "\t\"parameters\": {\n",
    "\t\t\"candidate_labels\": [\n",
    "\t\t\t\"refund\",\n",
    "\t\t\t\"legal\",\n",
    "\t\t\t\"faq\"\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d984c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db7eca",
   "metadata": {},
   "source": [
    "## Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference\n",
    "\n",
    "_This is an experimental feature, where the model will be loaded after the endpoint is created. This could lead to errors, e.g. models > 10GB_\n",
    "\n",
    "To deploy a model directly from the Hub to SageMaker we need to define 2 environment variables when creating the `HuggingFaceModel` . We need to define:\n",
    "\n",
    "- `HF_MODEL_ID`: defines the model id, which will be automatically loaded from [huggingface.co/models](http://huggingface.co/models) when creating or SageMaker Endpoint. The ðŸ¤— Hub provides +10 000 models all available through this environment variable.\n",
    "- `HF_TASK`: defines the task for the used ðŸ¤— Transformers pipeline. A full list of tasks can be find [here](https://huggingface.co/transformers/main_classes/pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n",
    "  'HF_TASK':'question-answering' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84c3f8",
   "metadata": {},
   "source": [
    "**Architecture**\n",
    "\n",
    "The [Hugging Face Inference Toolkit for SageMaker](https://github.com/aws/sagemaker-huggingface-inference-toolkit) is an open-source library for serving Hugging Face transformer models on SageMaker. It utilizes the SageMaker Inference Toolkit for starting up the model server, which is responsible for handling inference requests. The SageMaker Inference Toolkit uses [Multi Model Server (MMS)](https://github.com/awslabs/multi-model-server) for serving ML models. It bootstraps MMS with a configuration and settings that make it compatible with SageMaker and allow you to adjust important performance parameters, such as the number of workers per model, depending on the needs of your scenario.\n",
    "\n",
    "![](./imgs/hf-inference-toolkit.png)\n",
    "\n",
    "**Deploying a model using SageMaker hosting services is a three-step process:**\n",
    "\n",
    "1. **Create a model in SageMaker** â€”By creating a model, you tell SageMaker where it can find the model components. \n",
    "2. **Create an endpoint configuration for an HTTPS endpoint** â€”You specify the name of one or more models in production variants and the ML compute instances that you want SageMaker to launch to host each production variant.\n",
    "3. **Create an HTTPS endpoint** â€”Provide the endpoint configuration to SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration\n",
    "\n",
    "![](./imgs/sm-endpoint.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "\"inputs\": {\n",
    "    \"question\": \"What is used for inference?\",\n",
    "    \"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901166ce",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1bf7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
